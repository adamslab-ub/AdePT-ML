%% Generated by Sphinx.
\def\sphinxdocclass{report}
\documentclass[letterpaper,10pt,english]{sphinxmanual}
\ifdefined\pdfpxdimen
   \let\sphinxpxdimen\pdfpxdimen\else\newdimen\sphinxpxdimen
\fi \sphinxpxdimen=.75bp\relax
\ifdefined\pdfimageresolution
    \pdfimageresolution= \numexpr \dimexpr1in\relax/\sphinxpxdimen\relax
\fi
%% let collapsible pdf bookmarks panel have high depth per default
\PassOptionsToPackage{bookmarksdepth=5}{hyperref}

\PassOptionsToPackage{booktabs}{sphinx}
\PassOptionsToPackage{colorrows}{sphinx}

\PassOptionsToPackage{warn}{textcomp}
\usepackage[utf8]{inputenc}
\ifdefined\DeclareUnicodeCharacter
% support both utf8 and utf8x syntaxes
  \ifdefined\DeclareUnicodeCharacterAsOptional
    \def\sphinxDUC#1{\DeclareUnicodeCharacter{"#1}}
  \else
    \let\sphinxDUC\DeclareUnicodeCharacter
  \fi
  \sphinxDUC{00A0}{\nobreakspace}
  \sphinxDUC{2500}{\sphinxunichar{2500}}
  \sphinxDUC{2502}{\sphinxunichar{2502}}
  \sphinxDUC{2514}{\sphinxunichar{2514}}
  \sphinxDUC{251C}{\sphinxunichar{251C}}
  \sphinxDUC{2572}{\textbackslash}
\fi
\usepackage{cmap}
\usepackage[T1]{fontenc}
\usepackage{amsmath,amssymb,amstext}
\usepackage{babel}



\usepackage{tgtermes}
\usepackage{tgheros}
\renewcommand{\ttdefault}{txtt}



\usepackage[Bjarne]{fncychap}
\usepackage{sphinx}

\fvset{fontsize=auto}
\usepackage{geometry}


% Include hyperref last.
\usepackage{hyperref}
% Fix anchor placement for figures with captions.
\usepackage{hypcap}% it must be loaded after hyperref.
% Set up styles of URL: it should be placed after hyperref.
\urlstyle{same}

\addto\captionsenglish{\renewcommand{\contentsname}{Contents:}}

\usepackage{sphinxmessages}
\setcounter{tocdepth}{1}



\title{HyPyML}
\date{May 24, 2024}
\release{1.0}
\author{Manaswin Oddiraju}
\newcommand{\sphinxlogo}{\vbox{}}
\renewcommand{\releasename}{Release}
\makeindex
\begin{document}

\ifdefined\shorthandoff
  \ifnum\catcode`\=\string=\active\shorthandoff{=}\fi
  \ifnum\catcode`\"=\active\shorthandoff{"}\fi
\fi

\pagestyle{empty}
\sphinxmaketitle
\pagestyle{plain}
\sphinxtableofcontents
\pagestyle{normal}
\phantomsection\label{\detokenize{index::doc}}

\index{module@\spxentry{module}!hypyml.configs@\spxentry{hypyml.configs}}\index{hypyml.configs@\spxentry{hypyml.configs}!module@\spxentry{module}}\index{HybridConfig (class in hypyml.configs)@\spxentry{HybridConfig}\spxextra{class in hypyml.configs}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{index:hypyml.configs.HybridConfig}}
\pysigstartsignatures
\pysiglinewithargsret{\sphinxbfcode{\sphinxupquote{class\DUrole{w}{ }}}\sphinxcode{\sphinxupquote{hypyml.configs.}}\sphinxbfcode{\sphinxupquote{HybridConfig}}}{\sphinxparam{\DUrole{n}{models}\DUrole{p}{:}\DUrole{w}{ }\DUrole{n}{dict\DUrole{p}{{[}}str\DUrole{p}{,}\DUrole{w}{ }{\hyperref[\detokenize{index:hypyml.configs.MLPConfig}]{\sphinxcrossref{MLPConfig}}}\DUrole{w}{ }\DUrole{p}{|}\DUrole{w}{ }{\hyperref[\detokenize{index:hypyml.configs.PhysicsConfig}]{\sphinxcrossref{PhysicsConfig}}}\DUrole{w}{ }\DUrole{p}{|}\DUrole{w}{ }Module\DUrole{p}{{]}}}}\sphinxparamcomma \sphinxparam{\DUrole{n}{model\_inputs}\DUrole{p}{:}\DUrole{w}{ }\DUrole{n}{dict\DUrole{p}{{[}}str\DUrole{p}{,}\DUrole{w}{ }dict\DUrole{p}{{[}}str\DUrole{p}{,}\DUrole{w}{ }list\DUrole{p}{{[}}int\DUrole{p}{{]}}\DUrole{w}{ }\DUrole{p}{|}\DUrole{w}{ }None\DUrole{p}{{]}}\DUrole{p}{{]}}\DUrole{w}{ }\DUrole{p}{|}\DUrole{w}{ }None}\DUrole{w}{ }\DUrole{o}{=}\DUrole{w}{ }\DUrole{default_value}{None}}}{}
\pysigstopsignatures
\sphinxAtStartPar
Config for Ensemble Models.
\index{models (hypyml.configs.HybridConfig attribute)@\spxentry{models}\spxextra{hypyml.configs.HybridConfig attribute}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{index:hypyml.configs.HybridConfig.models}}
\pysigstartsignatures
\pysigline{\sphinxbfcode{\sphinxupquote{models}}}
\pysigstopsignatures
\sphinxAtStartPar
Contains Modelname as keys and an instance of ModelConfigs as values.
\begin{quote}\begin{description}
\sphinxlineitem{Type}
\sphinxAtStartPar
dict

\end{description}\end{quote}

\end{fulllineitems}

\index{model\_inputs (hypyml.configs.HybridConfig attribute)@\spxentry{model\_inputs}\spxextra{hypyml.configs.HybridConfig attribute}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{index:hypyml.configs.HybridConfig.model_inputs}}
\pysigstartsignatures
\pysigline{\sphinxbfcode{\sphinxupquote{model\_inputs}}}
\pysigstopsignatures
\sphinxAtStartPar
By default, the Ensemble model operates sequentially, using the output of the preceding model as input for the next.
Setting this dict to a non\sphinxhyphen{}empty value overrides that behavior.
Keys are model names; values are dicts specifying input customization.
Each inner dict holds model names as keys and specifies how to stack inputs:
\sphinxhyphen{} ‘None’ stacks the entire tensor.
\sphinxhyphen{} A list of ints stacks only specified dimensions.
Use “Input” if the input to this model matches the hybrid model’s original input.
\begin{quote}\begin{description}
\sphinxlineitem{Type}
\sphinxAtStartPar
dict

\end{description}\end{quote}

\end{fulllineitems}


\end{fulllineitems}

\index{MLPConfig (class in hypyml.configs)@\spxentry{MLPConfig}\spxextra{class in hypyml.configs}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{index:hypyml.configs.MLPConfig}}
\pysigstartsignatures
\pysiglinewithargsret{\sphinxbfcode{\sphinxupquote{class\DUrole{w}{ }}}\sphinxcode{\sphinxupquote{hypyml.configs.}}\sphinxbfcode{\sphinxupquote{MLPConfig}}}{\sphinxparam{\DUrole{n}{num\_input\_dim}\DUrole{p}{:}\DUrole{w}{ }\DUrole{n}{int}}\sphinxparamcomma \sphinxparam{\DUrole{n}{num\_hidden\_dim}\DUrole{p}{:}\DUrole{w}{ }\DUrole{n}{int}}\sphinxparamcomma \sphinxparam{\DUrole{n}{num\_output\_dim}\DUrole{p}{:}\DUrole{w}{ }\DUrole{n}{int}}\sphinxparamcomma \sphinxparam{\DUrole{n}{num\_hidden\_layers}\DUrole{p}{:}\DUrole{w}{ }\DUrole{n}{int}}\sphinxparamcomma \sphinxparam{\DUrole{n}{activation\_functions}\DUrole{p}{:}\DUrole{w}{ }\DUrole{n}{str}}}{}
\pysigstopsignatures
\sphinxAtStartPar
Configuration class for the Multilayer Perceptron (MLP) model.
\index{layers (hypyml.configs.MLPConfig attribute)@\spxentry{layers}\spxextra{hypyml.configs.MLPConfig attribute}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{index:hypyml.configs.MLPConfig.layers}}
\pysigstartsignatures
\pysigline{\sphinxbfcode{\sphinxupquote{layers}}}
\pysigstopsignatures
\sphinxAtStartPar
Total number of layers in the MLP (including hidden and output layers).
\begin{quote}\begin{description}
\sphinxlineitem{Type}
\sphinxAtStartPar
int

\end{description}\end{quote}

\end{fulllineitems}

\index{num\_input\_dim (hypyml.configs.MLPConfig attribute)@\spxentry{num\_input\_dim}\spxextra{hypyml.configs.MLPConfig attribute}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{index:hypyml.configs.MLPConfig.num_input_dim}}
\pysigstartsignatures
\pysigline{\sphinxbfcode{\sphinxupquote{num\_input\_dim}}}
\pysigstopsignatures
\sphinxAtStartPar
Number of input dimensions to the MLP.
\begin{quote}\begin{description}
\sphinxlineitem{Type}
\sphinxAtStartPar
int

\end{description}\end{quote}

\end{fulllineitems}

\index{num\_hidden\_dim (hypyml.configs.MLPConfig attribute)@\spxentry{num\_hidden\_dim}\spxextra{hypyml.configs.MLPConfig attribute}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{index:hypyml.configs.MLPConfig.num_hidden_dim}}
\pysigstartsignatures
\pysigline{\sphinxbfcode{\sphinxupquote{num\_hidden\_dim}}}
\pysigstopsignatures
\sphinxAtStartPar
Number of hidden dimensions in each hidden layer.
\begin{quote}\begin{description}
\sphinxlineitem{Type}
\sphinxAtStartPar
int

\end{description}\end{quote}

\end{fulllineitems}

\index{num\_output\_dim (hypyml.configs.MLPConfig attribute)@\spxentry{num\_output\_dim}\spxextra{hypyml.configs.MLPConfig attribute}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{index:hypyml.configs.MLPConfig.num_output_dim}}
\pysigstartsignatures
\pysigline{\sphinxbfcode{\sphinxupquote{num\_output\_dim}}}
\pysigstopsignatures
\sphinxAtStartPar
Number of output dimensions from the MLP.
\begin{quote}\begin{description}
\sphinxlineitem{Type}
\sphinxAtStartPar
int

\end{description}\end{quote}

\end{fulllineitems}

\index{num\_hidden\_layers (hypyml.configs.MLPConfig attribute)@\spxentry{num\_hidden\_layers}\spxextra{hypyml.configs.MLPConfig attribute}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{index:hypyml.configs.MLPConfig.num_hidden_layers}}
\pysigstartsignatures
\pysigline{\sphinxbfcode{\sphinxupquote{num\_hidden\_layers}}}
\pysigstopsignatures
\sphinxAtStartPar
Number of hidden layers in the MLP.
\begin{quote}\begin{description}
\sphinxlineitem{Type}
\sphinxAtStartPar
int

\end{description}\end{quote}

\end{fulllineitems}

\index{activation\_functions (hypyml.configs.MLPConfig attribute)@\spxentry{activation\_functions}\spxextra{hypyml.configs.MLPConfig attribute}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{index:hypyml.configs.MLPConfig.activation_functions}}
\pysigstartsignatures
\pysigline{\sphinxbfcode{\sphinxupquote{activation\_functions}}}
\pysigstopsignatures
\sphinxAtStartPar
String representation of the activation functions used in the MLP.
\begin{quote}\begin{description}
\sphinxlineitem{Type}
\sphinxAtStartPar
str

\end{description}\end{quote}

\end{fulllineitems}


\end{fulllineitems}

\index{PhysicsConfig (class in hypyml.configs)@\spxentry{PhysicsConfig}\spxextra{class in hypyml.configs}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{index:hypyml.configs.PhysicsConfig}}
\pysigstartsignatures
\pysiglinewithargsret{\sphinxbfcode{\sphinxupquote{class\DUrole{w}{ }}}\sphinxcode{\sphinxupquote{hypyml.configs.}}\sphinxbfcode{\sphinxupquote{PhysicsConfig}}}{\sphinxparam{\DUrole{n}{forward\_func}\DUrole{p}{:}\DUrole{w}{ }\DUrole{n}{Callable\DUrole{p}{{[}}\DUrole{p}{{[}}Tensor\DUrole{p}{{]}}\DUrole{p}{,}\DUrole{w}{ }Tensor\DUrole{p}{{]}}}}\sphinxparamcomma \sphinxparam{\DUrole{n}{jacobian\_func}\DUrole{p}{:}\DUrole{w}{ }\DUrole{n}{Callable\DUrole{p}{{[}}\DUrole{p}{{[}}Tensor\DUrole{p}{{]}}\DUrole{p}{,}\DUrole{w}{ }Tensor\DUrole{p}{{]}}}}}{}
\pysigstopsignatures
\sphinxAtStartPar
Configuration class for physics\sphinxhyphen{}related functions.
\index{forward\_func (hypyml.configs.PhysicsConfig attribute)@\spxentry{forward\_func}\spxextra{hypyml.configs.PhysicsConfig attribute}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{index:hypyml.configs.PhysicsConfig.forward_func}}
\pysigstartsignatures
\pysigline{\sphinxbfcode{\sphinxupquote{forward\_func}}}
\pysigstopsignatures
\sphinxAtStartPar
Forward function of the physics model.
\begin{quote}\begin{description}
\sphinxlineitem{Type}
\sphinxAtStartPar
Callable{[}{[}torch.Tensor{]}, torch.Tensor{]}

\end{description}\end{quote}

\end{fulllineitems}

\index{jacobian\_func (hypyml.configs.PhysicsConfig attribute)@\spxentry{jacobian\_func}\spxextra{hypyml.configs.PhysicsConfig attribute}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{index:hypyml.configs.PhysicsConfig.jacobian_func}}
\pysigstartsignatures
\pysigline{\sphinxbfcode{\sphinxupquote{jacobian\_func}}}
\pysigstopsignatures
\sphinxAtStartPar
Function to compute the Jacobian of the physics model.
\begin{quote}\begin{description}
\sphinxlineitem{Type}
\sphinxAtStartPar
Callable{[}{[}torch.Tensor{]}, torch.Tensor{]}

\end{description}\end{quote}

\end{fulllineitems}


\end{fulllineitems}

\index{module@\spxentry{module}!hypyml.models@\spxentry{hypyml.models}}\index{hypyml.models@\spxentry{hypyml.models}!module@\spxentry{module}}\index{MLP (class in hypyml.models)@\spxentry{MLP}\spxextra{class in hypyml.models}}\phantomsection\label{\detokenize{index:module-hypyml.models}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{index:hypyml.models.MLP}}
\pysigstartsignatures
\pysiglinewithargsret{\sphinxbfcode{\sphinxupquote{class\DUrole{w}{ }}}\sphinxcode{\sphinxupquote{hypyml.models.}}\sphinxbfcode{\sphinxupquote{MLP}}}{\sphinxparam{\DUrole{n}{config}\DUrole{p}{:}\DUrole{w}{ }\DUrole{n}{{\hyperref[\detokenize{index:hypyml.configs.MLPConfig}]{\sphinxcrossref{MLPConfig}}}}}}{}
\pysigstopsignatures
\sphinxAtStartPar
Multilayer Perceptron (MLP) neural network model.
\index{config (hypyml.models.MLP attribute)@\spxentry{config}\spxextra{hypyml.models.MLP attribute}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{index:hypyml.models.MLP.config}}
\pysigstartsignatures
\pysigline{\sphinxbfcode{\sphinxupquote{config}}}
\pysigstopsignatures\begin{quote}\begin{description}
\sphinxlineitem{Type}
\sphinxAtStartPar
Instance of MLPConfig dataclass.

\end{description}\end{quote}

\end{fulllineitems}


\begin{sphinxadmonition}{note}{Note:}
\sphinxAtStartPar
This class implements a Multilayer Perceptron (MLP) neural network model.
It takes a configuration dictionary with parameters such as hidden layer size,
input and output dimensions, and the number of hidden layers.
\end{sphinxadmonition}
\index{forward() (hypyml.models.MLP method)@\spxentry{forward()}\spxextra{hypyml.models.MLP method}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{index:hypyml.models.MLP.forward}}
\pysigstartsignatures
\pysiglinewithargsret{\sphinxbfcode{\sphinxupquote{forward}}}{\sphinxparam{\DUrole{n}{x}}}{}
\pysigstopsignatures
\sphinxAtStartPar
Forward pass of the MLP model.
\begin{quote}\begin{description}
\sphinxlineitem{Parameters}
\sphinxAtStartPar
\sphinxstyleliteralstrong{\sphinxupquote{x}} (\sphinxstyleliteralemphasis{\sphinxupquote{torch.Tensor}}) \textendash{} Input tensor.

\sphinxlineitem{Returns}
\sphinxAtStartPar
Output tensor.

\sphinxlineitem{Return type}
\sphinxAtStartPar
torch.Tensor

\end{description}\end{quote}

\end{fulllineitems}


\end{fulllineitems}

\index{Physics (class in hypyml.models)@\spxentry{Physics}\spxextra{class in hypyml.models}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{index:hypyml.models.Physics}}
\pysigstartsignatures
\pysiglinewithargsret{\sphinxbfcode{\sphinxupquote{class\DUrole{w}{ }}}\sphinxcode{\sphinxupquote{hypyml.models.}}\sphinxbfcode{\sphinxupquote{Physics}}}{\sphinxparam{\DUrole{o}{*}\DUrole{n}{args}}\sphinxparamcomma \sphinxparam{\DUrole{o}{**}\DUrole{n}{kwargs}}}{}
\pysigstopsignatures
\sphinxAtStartPar
Custom Autograd function to enable backpropagation on Custom Physics Models.

\sphinxAtStartPar
Attributes:
config: Instance of PhysicsConfig.
\index{backward() (hypyml.models.Physics static method)@\spxentry{backward()}\spxextra{hypyml.models.Physics static method}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{index:hypyml.models.Physics.backward}}
\pysigstartsignatures
\pysiglinewithargsret{\sphinxbfcode{\sphinxupquote{static\DUrole{w}{ }}}\sphinxbfcode{\sphinxupquote{backward}}}{\sphinxparam{\DUrole{n}{ctx}}\sphinxparamcomma \sphinxparam{\DUrole{n}{grad\_output}}}{}
\pysigstopsignatures
\sphinxAtStartPar
Define a formula for differentiating the operation with backward mode automatic differentiation.

\sphinxAtStartPar
This function is to be overridden by all subclasses.
(Defining this function is equivalent to defining the \sphinxcode{\sphinxupquote{vjp}} function.)

\sphinxAtStartPar
It must accept a context \sphinxcode{\sphinxupquote{ctx}} as the first argument, followed by
as many outputs as the {\hyperref[\detokenize{index:hypyml.models.Physics.forward}]{\sphinxcrossref{\sphinxcode{\sphinxupquote{forward()}}}}} returned (None will be passed in
for non tensor outputs of the forward function),
and it should return as many tensors, as there were inputs to
{\hyperref[\detokenize{index:hypyml.models.Physics.forward}]{\sphinxcrossref{\sphinxcode{\sphinxupquote{forward()}}}}}. Each argument is the gradient w.r.t the given output,
and each returned value should be the gradient w.r.t. the
corresponding input. If an input is not a Tensor or is a Tensor not
requiring grads, you can just pass None as a gradient for that input.

\sphinxAtStartPar
The context can be used to retrieve tensors saved during the forward
pass. It also has an attribute \sphinxcode{\sphinxupquote{ctx.needs\_input\_grad}} as a tuple
of booleans representing whether each input needs gradient. E.g.,
{\hyperref[\detokenize{index:hypyml.models.Physics.backward}]{\sphinxcrossref{\sphinxcode{\sphinxupquote{backward()}}}}} will have \sphinxcode{\sphinxupquote{ctx.needs\_input\_grad{[}0{]} = True}} if the
first input to {\hyperref[\detokenize{index:hypyml.models.Physics.forward}]{\sphinxcrossref{\sphinxcode{\sphinxupquote{forward()}}}}} needs gradient computed w.r.t. the
output.

\end{fulllineitems}

\index{forward() (hypyml.models.Physics static method)@\spxentry{forward()}\spxextra{hypyml.models.Physics static method}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{index:hypyml.models.Physics.forward}}
\pysigstartsignatures
\pysiglinewithargsret{\sphinxbfcode{\sphinxupquote{static\DUrole{w}{ }}}\sphinxbfcode{\sphinxupquote{forward}}}{\sphinxparam{\DUrole{n}{ctx}}\sphinxparamcomma \sphinxparam{\DUrole{n}{x}}\sphinxparamcomma \sphinxparam{\DUrole{n}{forward\_fun}}\sphinxparamcomma \sphinxparam{\DUrole{n}{jacobian\_fun}}\sphinxparamcomma \sphinxparam{\DUrole{n}{args}\DUrole{o}{=}\DUrole{default_value}{None}}}{}
\pysigstopsignatures
\sphinxAtStartPar
Define the forward of the custom autograd Function.

\sphinxAtStartPar
This function is to be overridden by all subclasses.
There are two ways to define forward:

\sphinxAtStartPar
Usage 1 (Combined forward and ctx):

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n+nd}{@staticmethod}
\PYG{k}{def} \PYG{n+nf}{forward}\PYG{p}{(}\PYG{n}{ctx}\PYG{p}{:} \PYG{n}{Any}\PYG{p}{,} \PYG{o}{*}\PYG{n}{args}\PYG{p}{:} \PYG{n}{Any}\PYG{p}{,} \PYG{o}{*}\PYG{o}{*}\PYG{n}{kwargs}\PYG{p}{:} \PYG{n}{Any}\PYG{p}{)} \PYG{o}{\PYGZhy{}}\PYG{o}{\PYGZgt{}} \PYG{n}{Any}\PYG{p}{:}
    \PYG{k}{pass}
\end{sphinxVerbatim}
\begin{itemize}
\item {} 
\sphinxAtStartPar
It must accept a context ctx as the first argument, followed by any
number of arguments (tensors or other types).

\item {} 
\sphinxAtStartPar
See \DUrole{xref,std,std-ref}{combining\sphinxhyphen{}forward\sphinxhyphen{}context} for more details

\end{itemize}

\sphinxAtStartPar
Usage 2 (Separate forward and ctx):

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n+nd}{@staticmethod}
\PYG{k}{def} \PYG{n+nf}{forward}\PYG{p}{(}\PYG{o}{*}\PYG{n}{args}\PYG{p}{:} \PYG{n}{Any}\PYG{p}{,} \PYG{o}{*}\PYG{o}{*}\PYG{n}{kwargs}\PYG{p}{:} \PYG{n}{Any}\PYG{p}{)} \PYG{o}{\PYGZhy{}}\PYG{o}{\PYGZgt{}} \PYG{n}{Any}\PYG{p}{:}
    \PYG{k}{pass}

\PYG{n+nd}{@staticmethod}
\PYG{k}{def} \PYG{n+nf}{setup\PYGZus{}context}\PYG{p}{(}\PYG{n}{ctx}\PYG{p}{:} \PYG{n}{Any}\PYG{p}{,} \PYG{n}{inputs}\PYG{p}{:} \PYG{n}{Tuple}\PYG{p}{[}\PYG{n}{Any}\PYG{p}{,} \PYG{o}{.}\PYG{o}{.}\PYG{o}{.}\PYG{p}{]}\PYG{p}{,} \PYG{n}{output}\PYG{p}{:} \PYG{n}{Any}\PYG{p}{)} \PYG{o}{\PYGZhy{}}\PYG{o}{\PYGZgt{}} \PYG{k+kc}{None}\PYG{p}{:}
    \PYG{k}{pass}
\end{sphinxVerbatim}
\begin{itemize}
\item {} 
\sphinxAtStartPar
The forward no longer accepts a ctx argument.

\item {} 
\sphinxAtStartPar
Instead, you must also override the \sphinxcode{\sphinxupquote{torch.autograd.Function.setup\_context()}}
staticmethod to handle setting up the \sphinxcode{\sphinxupquote{ctx}} object.
\sphinxcode{\sphinxupquote{output}} is the output of the forward, \sphinxcode{\sphinxupquote{inputs}} are a Tuple of inputs
to the forward.

\item {} 
\sphinxAtStartPar
See \DUrole{xref,std,std-ref}{extending\sphinxhyphen{}autograd} for more details

\end{itemize}

\sphinxAtStartPar
The context can be used to store arbitrary data that can be then
retrieved during the backward pass. Tensors should not be stored
directly on \sphinxtitleref{ctx} (though this is not currently enforced for
backward compatibility). Instead, tensors should be saved either with
\sphinxcode{\sphinxupquote{ctx.save\_for\_backward()}} if they are intended to be used in
\sphinxcode{\sphinxupquote{backward}} (equivalently, \sphinxcode{\sphinxupquote{vjp}}) or \sphinxcode{\sphinxupquote{ctx.save\_for\_forward()}}
if they are intended to be used for in \sphinxcode{\sphinxupquote{jvp}}.

\end{fulllineitems}


\end{fulllineitems}

\index{module@\spxentry{module}!hypyml.ensemble@\spxentry{hypyml.ensemble}}\index{hypyml.ensemble@\spxentry{hypyml.ensemble}!module@\spxentry{module}}\index{HybridModel (class in hypyml.ensemble)@\spxentry{HybridModel}\spxextra{class in hypyml.ensemble}}\phantomsection\label{\detokenize{index:module-hypyml.ensemble}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{index:hypyml.ensemble.HybridModel}}
\pysigstartsignatures
\pysiglinewithargsret{\sphinxbfcode{\sphinxupquote{class\DUrole{w}{ }}}\sphinxcode{\sphinxupquote{hypyml.ensemble.}}\sphinxbfcode{\sphinxupquote{HybridModel}}}{\sphinxparam{\DUrole{n}{config}\DUrole{p}{:}\DUrole{w}{ }\DUrole{n}{{\hyperref[\detokenize{index:hypyml.configs.HybridConfig}]{\sphinxcrossref{HybridConfig}}}}}}{}
\pysigstopsignatures
\sphinxAtStartPar
Torch Module for Serial Hybrid Physics Models.
\begin{quote}\begin{description}
\sphinxlineitem{Parameters}\begin{itemize}
\item {} 
\sphinxAtStartPar
\sphinxstyleliteralstrong{\sphinxupquote{models\_mlp}} (\sphinxstyleliteralemphasis{\sphinxupquote{Torch module list}}\sphinxstyleliteralemphasis{\sphinxupquote{ of }}\sphinxstyleliteralemphasis{\sphinxupquote{all MLP modules.}})

\item {} 
\sphinxAtStartPar
\sphinxstyleliteralstrong{\sphinxupquote{models\_cnn}} (\sphinxstyleliteralemphasis{\sphinxupquote{Torch module list}}\sphinxstyleliteralemphasis{\sphinxupquote{ of }}\sphinxstyleliteralemphasis{\sphinxupquote{all CNN modules.}})

\item {} 
\sphinxAtStartPar
\sphinxstyleliteralstrong{\sphinxupquote{models\_physics}} (\sphinxstyleliteralemphasis{\sphinxupquote{Torch module list}}\sphinxstyleliteralemphasis{\sphinxupquote{ of }}\sphinxstyleliteralemphasis{\sphinxupquote{all Physics modules.}})

\item {} 
\sphinxAtStartPar
\sphinxstyleliteralstrong{\sphinxupquote{unmodified\_inputs}} (\sphinxstyleliteralemphasis{\sphinxupquote{Indices}}\sphinxstyleliteralemphasis{\sphinxupquote{ of }}\sphinxstyleliteralemphasis{\sphinxupquote{the inputs that are to be passed directly to the}})

\item {} 
\sphinxAtStartPar
\sphinxstyleliteralstrong{\sphinxupquote{model.}} (\sphinxstyleliteralemphasis{\sphinxupquote{model. These are appended to the outputs}}\sphinxstyleliteralemphasis{\sphinxupquote{ of }}\sphinxstyleliteralemphasis{\sphinxupquote{the previous}})

\item {} 
\sphinxAtStartPar
\sphinxstyleliteralstrong{\sphinxupquote{architecture}} (\sphinxstyleliteralemphasis{\sphinxupquote{Dict with key corresponding to model name and value being a model}})

\item {} 
\sphinxAtStartPar
\sphinxstyleliteralstrong{\sphinxupquote{config.}}

\end{itemize}

\end{description}\end{quote}
\index{forward() (hypyml.ensemble.HybridModel method)@\spxentry{forward()}\spxextra{hypyml.ensemble.HybridModel method}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{index:hypyml.ensemble.HybridModel.forward}}
\pysigstartsignatures
\pysiglinewithargsret{\sphinxbfcode{\sphinxupquote{forward}}}{\sphinxparam{\DUrole{n}{x}}\sphinxparamcomma \sphinxparam{\DUrole{n}{phy\_args}\DUrole{o}{=}\DUrole{default_value}{None}}}{}
\pysigstopsignatures
\sphinxAtStartPar
Function to run inference on the hybrid model.

\end{fulllineitems}


\end{fulllineitems}

\index{module@\spxentry{module}!hypyml.train\_utils@\spxentry{hypyml.train\_utils}}\index{hypyml.train\_utils@\spxentry{hypyml.train\_utils}!module@\spxentry{module}}\index{train() (in module hypyml.train\_utils)@\spxentry{train()}\spxextra{in module hypyml.train\_utils}}\phantomsection\label{\detokenize{index:module-hypyml.train_utils}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{index:hypyml.train_utils.train}}
\pysigstartsignatures
\pysiglinewithargsret{\sphinxcode{\sphinxupquote{hypyml.train\_utils.}}\sphinxbfcode{\sphinxupquote{train}}}{\sphinxparam{\DUrole{n}{model}}\sphinxparamcomma \sphinxparam{\DUrole{n}{train\_loader}}\sphinxparamcomma \sphinxparam{\DUrole{n}{test\_loader}}\sphinxparamcomma \sphinxparam{\DUrole{n}{optimizer}}\sphinxparamcomma \sphinxparam{\DUrole{n}{loss\_fn}}\sphinxparamcomma \sphinxparam{\DUrole{n}{scheduler}}\sphinxparamcomma \sphinxparam{\DUrole{n}{filename}}\sphinxparamcomma \sphinxparam{\DUrole{n}{epochs}}\sphinxparamcomma \sphinxparam{\DUrole{n}{print\_training\_loss}\DUrole{o}{=}\DUrole{default_value}{True}}\sphinxparamcomma \sphinxparam{\DUrole{n}{save\_frequency}\DUrole{o}{=}\DUrole{default_value}{50}}}{}
\pysigstopsignatures
\sphinxAtStartPar
Training Function.
\begin{quote}\begin{description}
\sphinxlineitem{Parameters}\begin{itemize}
\item {} 
\sphinxAtStartPar
\sphinxstyleliteralstrong{\sphinxupquote{train\_loader}} (\sphinxstyleliteralemphasis{\sphinxupquote{torch.Torch\_Dataloader}}) \textendash{} Torch Dataloader with training samples.

\item {} 
\sphinxAtStartPar
\sphinxstyleliteralstrong{\sphinxupquote{test\_loader}} (\sphinxstyleliteralemphasis{\sphinxupquote{torch.Torch\_Dataloader}}) \textendash{} Torch Dataloader with validation samples.

\item {} 
\sphinxAtStartPar
\sphinxstyleliteralstrong{\sphinxupquote{optimizer}} (\sphinxstyleliteralemphasis{\sphinxupquote{torch.optim.Optimizer}}) \textendash{} Initialized Torch Optimizer.

\item {} 
\sphinxAtStartPar
\sphinxstyleliteralstrong{\sphinxupquote{loss\_fn}} (\sphinxstyleliteralemphasis{\sphinxupquote{callable}}) \textendash{} Loss function for training.

\item {} 
\sphinxAtStartPar
\sphinxstyleliteralstrong{\sphinxupquote{scheduler}} (\sphinxstyleliteralemphasis{\sphinxupquote{torch.optim.lr\_scheduler}}) \textendash{} Learning rate scheduler.

\item {} 
\sphinxAtStartPar
\sphinxstyleliteralstrong{\sphinxupquote{filename}} (\sphinxstyleliteralemphasis{\sphinxupquote{str}}) \textendash{} File name for saving the trained model.

\item {} 
\sphinxAtStartPar
\sphinxstyleliteralstrong{\sphinxupquote{epochs}} (\sphinxstyleliteralemphasis{\sphinxupquote{int}}) \textendash{} Number of training epochs.

\item {} 
\sphinxAtStartPar
\sphinxstyleliteralstrong{\sphinxupquote{print\_training\_loss}} (\sphinxstyleliteralemphasis{\sphinxupquote{bool}}) \textendash{} Option to toggle printing epoch loss.

\item {} 
\sphinxAtStartPar
\sphinxstyleliteralstrong{\sphinxupquote{save\_frequency}} (\sphinxstyleliteralemphasis{\sphinxupquote{int}}) \textendash{} Number of epochs per which to save the model parameters to disk.

\end{itemize}

\sphinxlineitem{Return type}
\sphinxAtStartPar
Trained Hybrid Model.

\end{description}\end{quote}

\end{fulllineitems}



\chapter{Indices and tables}
\label{\detokenize{index:indices-and-tables}}\begin{itemize}
\item {} 
\sphinxAtStartPar
\DUrole{xref,std,std-ref}{genindex}

\item {} 
\sphinxAtStartPar
\DUrole{xref,std,std-ref}{modindex}

\item {} 
\sphinxAtStartPar
\DUrole{xref,std,std-ref}{search}

\end{itemize}


\renewcommand{\indexname}{Python Module Index}
\begin{sphinxtheindex}
\let\bigletter\sphinxstyleindexlettergroup
\bigletter{h}
\item\relax\sphinxstyleindexentry{hypyml.configs}\sphinxstyleindexpageref{index:\detokenize{module-hypyml.configs}}
\item\relax\sphinxstyleindexentry{hypyml.ensemble}\sphinxstyleindexpageref{index:\detokenize{module-hypyml.ensemble}}
\item\relax\sphinxstyleindexentry{hypyml.models}\sphinxstyleindexpageref{index:\detokenize{module-hypyml.models}}
\item\relax\sphinxstyleindexentry{hypyml.train\_utils}\sphinxstyleindexpageref{index:\detokenize{module-hypyml.train_utils}}
\end{sphinxtheindex}

\renewcommand{\indexname}{Index}
\printindex
\end{document}